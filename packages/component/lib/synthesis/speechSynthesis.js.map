{"version":3,"sources":["../../src/synthesis/speechSynthesis.js"],"names":["DEFAULT_REGION","DEFAULT_OUTPUT_FORMAT","SpeechSynthesis","onvoiceschanged","region","outputFormat","queue","AudioContextQueue","stop","utterance","SpeechSynthesisUtterance","Error","fetchToken","accessToken","Promise","resolve","reject","addEventListener","preload","push"],"mappings":";;;;;;;;;;;;;;;;;AAAA;;AACA;;AACA;;AAEA,IAAMA,cAAc,GAAG,QAAvB,C,CACA;;AACA,IAAMC,qBAAqB,GAAG,kCAA9B;;IAEMC,e;;;AACJ,6BAAc;AAAA;AACZ,SAAKC,eAAL,GAAuB,IAAvB;AACA,SAAKC,MAAL,GAAcJ,cAAd;AACA,SAAKK,YAAL,GAAoBJ,qBAApB;AACA,SAAKK,KAAL,GAAa,IAAIC,0BAAJ,EAAb;AACD;;;;6BAEQ;AACP,WAAKD,KAAL,CAAWE,IAAX;AACD;;;gCAEW;AACV,aAAO,2BAAP;AACD;;;;;;iDAEWC,S;;;;;;;;oBACJA,SAAS,YAAYC,iC;;;;;sBACnB,IAAIC,KAAJ,CAAU,mBAAV,C;;;oBAGH,KAAKC,U;;;;;sBACF,IAAID,KAAJ,CAAU,yCAAV,C;;;sBACG,OAAO,KAAKC,UAAZ,KAA2B,U;;;;;sBAC9B,IAAID,KAAJ,CAAU,mHAAV,C;;;;uBAGkB,KAAKC,UAAL,E;;;AAApBC,gBAAAA,W;iDAEC,IAAIC,OAAJ,CAAY,UAACC,OAAD,EAAUC,MAAV,EAAqB;AACtCP,kBAAAA,SAAS,CAACQ,gBAAV,CAA2B,KAA3B,EAAkCF,OAAlC;AACAN,kBAAAA,SAAS,CAACQ,gBAAV,CAA2B,OAA3B,EAAoCD,MAApC;AACAP,kBAAAA,SAAS,CAACI,WAAV,GAAwBA,WAAxB;AACAJ,kBAAAA,SAAS,CAACL,MAAV,GAAmB,KAAI,CAACA,MAAxB;AACAK,kBAAAA,SAAS,CAACJ,YAAV,GAAyB,KAAI,CAACA,YAA9B;AACAI,kBAAAA,SAAS,CAACS,OAAV;;AAEA,kBAAA,KAAI,CAACZ,KAAL,CAAWa,IAAX,CAAgBV,SAAhB;AACD,iBATM,C;;;;;;;;;;;;;;;;;;eAaI,IAAIP,eAAJ,E","sourcesContent":["import AudioContextQueue from './AudioContextQueue';\nimport fetchVoices from './fetchVoices';\nimport SpeechSynthesisUtterance from './SpeechSynthesisUtterance';\n\nconst DEFAULT_REGION = 'westus';\n// Supported output format can be found at https://docs.microsoft.com/en-us/azure/cognitive-services/Speech/API-Reference-REST/BingVoiceOutput#Subscription\nconst DEFAULT_OUTPUT_FORMAT = 'audio-16khz-128kbitrate-mono-mp3';\n\nclass SpeechSynthesis {\n  constructor() {\n    this.onvoiceschanged = null;\n    this.region = DEFAULT_REGION;\n    this.outputFormat = DEFAULT_OUTPUT_FORMAT;\n    this.queue = new AudioContextQueue();\n  }\n\n  cancel() {\n    this.queue.stop();\n  }\n\n  getVoices() {\n    return fetchVoices();\n  }\n\n  async speak(utterance) {\n    if (!(utterance instanceof SpeechSynthesisUtterance)) {\n      throw new Error('invalid utterance');\n    }\n\n    if (!this.fetchToken) {\n      throw new Error('SpeechSynthesis: fetchToken must be set');\n    } else if (typeof this.fetchToken !== 'function') {\n      throw new Error('SpeechSynthesis: fetchToken must be a function that returns a Promise and it will resolve to a string-based token');\n    }\n\n    const accessToken = await this.fetchToken();\n\n    return new Promise((resolve, reject) => {\n      utterance.addEventListener('end', resolve);\n      utterance.addEventListener('error', reject);\n      utterance.accessToken = accessToken;\n      utterance.region = this.region;\n      utterance.outputFormat = this.outputFormat;\n      utterance.preload();\n\n      this.queue.push(utterance);\n    });\n  }\n}\n\nexport default new SpeechSynthesis()\n"],"file":"speechSynthesis.js"}